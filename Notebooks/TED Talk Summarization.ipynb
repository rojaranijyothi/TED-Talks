{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization:\n",
    "\n",
    "Text summarization can broadly be divided into two categories \n",
    "- Extractive Summarization and \n",
    "- Abstractive Summarization\n",
    "\n",
    "\n",
    "- **Extractive Summarization**: \n",
    "\n",
    "    These methods rely on extracting several parts, such as phrases and sentences, from a piece of text and stack them together to create a summary. Therefore, identifying the right sentences for summarization is of utmost importance in an extractive method.\n",
    "    \n",
    "    \n",
    "- **Abstractive Summarization**: \n",
    "    \n",
    "    These methods use advanced NLP techniques to generate an entirely new summary. Some parts of this summary may not even appear in the original text.\n",
    "    \n",
    "Here I am going to focuse on the extractive summarization technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the necessary libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import wordcloud\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>transcript</th>\n",
       "      <th>duration</th>\n",
       "      <th>film_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sent</th>\n",
       "      <th>sentiment_lab</th>\n",
       "      <th>clean_transc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>2006-06-26</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>3066</td>\n",
       "      <td>14344</td>\n",
       "      <td>225</td>\n",
       "      <td>4.678408</td>\n",
       "      <td>13.626667</td>\n",
       "      <td>0.146452</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>good morning great blow away thing fact leave ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>16.283333</td>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>2006-06-26</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>2089</td>\n",
       "      <td>9726</td>\n",
       "      <td>141</td>\n",
       "      <td>4.655816</td>\n",
       "      <td>14.815603</td>\n",
       "      <td>0.157775</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>21.433333</td>\n",
       "      <td>2006-02-23</td>\n",
       "      <td>2006-06-26</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>3253</td>\n",
       "      <td>15057</td>\n",
       "      <td>256</td>\n",
       "      <td>4.628650</td>\n",
       "      <td>12.707031</td>\n",
       "      <td>0.136579</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>hello voice mail old friend tech support ignor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Majora Carter: Greening the ghetto</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>Activist for environmental justice</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-26</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>3015</td>\n",
       "      <td>15235</td>\n",
       "      <td>181</td>\n",
       "      <td>5.053068</td>\n",
       "      <td>16.657459</td>\n",
       "      <td>0.082928</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>today happy hear sustainable development save ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hans Rosling: The best stats you've ever seen</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>Global health expert; data visionary</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>2006-02-21</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>3121</td>\n",
       "      <td>14245</td>\n",
       "      <td>236</td>\n",
       "      <td>4.564242</td>\n",
       "      <td>13.224576</td>\n",
       "      <td>0.096483</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>year ago task teach global development swedish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name  \\\n",
       "0      Ken Robinson: Do schools kill creativity?   \n",
       "1           Al Gore: Averting the climate crisis   \n",
       "2                  David Pogue: Simplicity sells   \n",
       "3             Majora Carter: Greening the ghetto   \n",
       "4  Hans Rosling: The best stats you've ever seen   \n",
       "\n",
       "                             title  \\\n",
       "0      Do schools kill creativity?   \n",
       "1      Averting the climate crisis   \n",
       "2                 Simplicity sells   \n",
       "3              Greening the ghetto   \n",
       "4  The best stats you've ever seen   \n",
       "\n",
       "                                         description   main_speaker  \\\n",
       "0  Sir Ken Robinson makes an entertaining and pro...   Ken Robinson   \n",
       "1  With the same humor and humanity he exuded in ...        Al Gore   \n",
       "2  New York Times columnist David Pogue takes aim...    David Pogue   \n",
       "3  In an emotionally charged talk, MacArthur-winn...  Majora Carter   \n",
       "4  You've never seen data presented like this. Wi...   Hans Rosling   \n",
       "\n",
       "                     speaker_occupation  \\\n",
       "0                       Author/educator   \n",
       "1                      Climate advocate   \n",
       "2                  Technology columnist   \n",
       "3    Activist for environmental justice   \n",
       "4  Global health expert; data visionary   \n",
       "\n",
       "                                          transcript   duration   film_date  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...  19.400000  2006-02-24   \n",
       "1  Thank you so much, Chris. And it's truly a gre...  16.283333  2006-02-24   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...  21.433333  2006-02-23   \n",
       "3  If you're here today — and I'm very happy that...  18.600000  2006-02-25   \n",
       "4  About 10 years ago, I took on the task to teac...  19.833333  2006-02-21   \n",
       "\n",
       "  published_date  languages  ...  word_count char_count  sentence_count  \\\n",
       "0     2006-06-26         60  ...        3066      14344             225   \n",
       "1     2006-06-26         43  ...        2089       9726             141   \n",
       "2     2006-06-26         26  ...        3253      15057             256   \n",
       "3     2006-06-26         35  ...        3015      15235             181   \n",
       "4     2006-06-27         48  ...        3121      14245             236   \n",
       "\n",
       "  avg_word_length  avg_sentence_length sentiment sentiment_label      sent  \\\n",
       "0        4.678408            13.626667  0.146452        positive  positive   \n",
       "1        4.655816            14.815603  0.157775        positive  positive   \n",
       "2        4.628650            12.707031  0.136579        positive  positive   \n",
       "3        5.053068            16.657459  0.082928        positive  positive   \n",
       "4        4.564242            13.224576  0.096483        positive  positive   \n",
       "\n",
       "   sentiment_lab                                       clean_transc  \n",
       "0       positive  good morning great blow away thing fact leave ...  \n",
       "1       positive  thank chris truly great honor opportunity come...  \n",
       "2       positive  hello voice mail old friend tech support ignor...  \n",
       "3       positive  today happy hear sustainable development save ...  \n",
       "4       positive  year ago task teach global development swedish...  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "ted_talks = pd.read_csv(\"/Users/HOME/Desktop/Springboard/TED-Talks/Data/clean_transcript_1.csv\",index_col=0)\n",
    "ted_talks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talks = ted_talks[~(ted_talks['clean_transc'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talks = ted_talks.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talks = ted_talks[['title','description','transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>What we're missing in the debate about immigra...</td>\n",
       "      <td>Between 2008 and 2016, the United States depor...</td>\n",
       "      <td>So, Ma was trying to explain something to me a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>The most Martian place on Earth</td>\n",
       "      <td>How can you study Mars without a spaceship? He...</td>\n",
       "      <td>This is a picture of a sunset on Mars taken by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>What intelligent machines can learn from a sch...</td>\n",
       "      <td>Science fiction visions of the future show us ...</td>\n",
       "      <td>In my early days as a graduate student, I went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>A black man goes undercover in the alt-right</td>\n",
       "      <td>In an unmissable talk about race and politics ...</td>\n",
       "      <td>I took a cell phone and accidentally made myse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>How a video game might help us build better ci...</td>\n",
       "      <td>With more than half of the world population li...</td>\n",
       "      <td>We humans are becoming an urban species, so ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2455 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                           Do schools kill creativity?   \n",
       "1                           Averting the climate crisis   \n",
       "2                                      Simplicity sells   \n",
       "3                                   Greening the ghetto   \n",
       "4                       The best stats you've ever seen   \n",
       "...                                                 ...   \n",
       "2450  What we're missing in the debate about immigra...   \n",
       "2451                    The most Martian place on Earth   \n",
       "2452  What intelligent machines can learn from a sch...   \n",
       "2453       A black man goes undercover in the alt-right   \n",
       "2454  How a video game might help us build better ci...   \n",
       "\n",
       "                                            description  \\\n",
       "0     Sir Ken Robinson makes an entertaining and pro...   \n",
       "1     With the same humor and humanity he exuded in ...   \n",
       "2     New York Times columnist David Pogue takes aim...   \n",
       "3     In an emotionally charged talk, MacArthur-winn...   \n",
       "4     You've never seen data presented like this. Wi...   \n",
       "...                                                 ...   \n",
       "2450  Between 2008 and 2016, the United States depor...   \n",
       "2451  How can you study Mars without a spaceship? He...   \n",
       "2452  Science fiction visions of the future show us ...   \n",
       "2453  In an unmissable talk about race and politics ...   \n",
       "2454  With more than half of the world population li...   \n",
       "\n",
       "                                             transcript  \n",
       "0     Good morning. How are you?(Laughter)It's been ...  \n",
       "1     Thank you so much, Chris. And it's truly a gre...  \n",
       "2     (Music: \"The Sound of Silence,\" Simon & Garfun...  \n",
       "3     If you're here today — and I'm very happy that...  \n",
       "4     About 10 years ago, I took on the task to teac...  \n",
       "...                                                 ...  \n",
       "2450  So, Ma was trying to explain something to me a...  \n",
       "2451  This is a picture of a sunset on Mars taken by...  \n",
       "2452  In my early days as a graduate student, I went...  \n",
       "2453  I took a cell phone and accidentally made myse...  \n",
       "2454  We humans are becoming an urban species, so ci...  \n",
       "\n",
       "[2455 rows x 3 columns]"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = re.sub(r\"\\((.*?)\\)|—|\\\"|\\n|\\+\", r\" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text summarization using spaCy \n",
    "\n",
    "- spaCy is a free, open-source advanced natural language processing library, written in the programming languages Python and Cython. spaCy mainly used in the development of production software and also supports deep learning workflow via statistical models of PyTorch and TensorFlow.\n",
    "\n",
    "\n",
    "- spaCy provides a fast and accurate syntactic analysis, named entity recognition and ready access to word vectors. We can use the default word vectors or replace them with any you have. spaCy also offers tokenization, sentence boundary detection, POS tagging, syntactic parsing, integrated word vectors, and alignment into the original string with high accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text summarization using spaCy \n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def text_summary_spacy(text):\n",
    "    text = re.sub(r\"\\((.*?)\\)|—|\\\"|\\n|\\+\", r\" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    #Filtering tokens\n",
    "    keyword = []\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    for token in doc:\n",
    "        if(token.text in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    freq_word = Counter(keyword)\n",
    "    if not keyword:\n",
    "        print(\"This transcript is empty\")\n",
    "    else:\n",
    "        #Normalizing tokens\n",
    "        max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "\n",
    "        for word in freq_word.keys():  \n",
    "                freq_word[word] = (freq_word[word]/max_freq)\n",
    "\n",
    "        #Weighing sentences\n",
    "        sent_strength={}\n",
    "        for sent in doc.sents:\n",
    "            for word in sent:\n",
    "                if word.text in freq_word.keys():\n",
    "                    if sent in sent_strength.keys():\n",
    "                        sent_strength[sent]+=freq_word[word.text]\n",
    "                    else:\n",
    "                        sent_strength[sent]=freq_word[word.text]\n",
    "\n",
    "        #Summarizing the string\n",
    "        summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
    "        final_sentences = [ w.text for w in summarized_sentences ]\n",
    "        summary = ' '.join(final_sentences)\n",
    "        return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All life on Earth requires water, so in my case I focus on the intimate relationship between water and life in order to understand if we could find life in a planet as dry as Mars. So I remembered that I usually see fogs in Yungay, so after setting sensors in a number of places, where I remember never seeing fogs or clouds, I reported four other sites much drier than Yungay, with this one, María Elena South, being the truly driest place on Earth, as dry as Mars, and amazingly, just a 15-minute ride from the small mining town where I was born. Now, in this search, we were trying to actually find the dry limit for life on Earth, a place so dry that nothing was able to survive in it.'"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the summaraiztion for 2451th transcript in the dataset as an example.\n",
    "text_summary_spacy(ted_talks['transcript'][2451])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization using Gensim with TextRank\n",
    "\n",
    "- **gensim** is a very handy python library for performing NLP tasks.The text summarization process using gensim library is based on TextRank Algorithm.\n",
    "\n",
    "\n",
    "- **TextRank** is an extractive summarization technique.It is based on the concept that words which occur more frequently are significant.Hence,the sentences containing highly frequent words are important.\n",
    "\n",
    "Based on this,the algorithm assigns scores to each sentence in the text.The top-ranked sentences make it to the summary.\n",
    "\n",
    "The default parameters of the **summarize** function are:\n",
    "\n",
    "**ratio**: It can take values between 0 to 1. It represents the proportion of the summary compared to the original text.\n",
    "\n",
    "**word_count**: It decides the no of words in the summary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim_sum_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this place, we reported a new type of microalgae that grew only on top of the spiderwebs that covered the cave entrance.\\nIt's covered with dew, so this microalgae learned that in order to carry photosynthesis in the coast of the driest desert on Earth, they could use the spiderwebs.\\nThese type of findings suggest to me that on Mars, we may find even photosynthetic life inside caves.\\nBut even here, well hidden underground, we found a number of different microorganisms, which suggested to me that similarly dry places, like Mars, may be in inhabited.\""
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "text = re.sub(r\"\\((.*?)\\)|—|\\\"|\\n|\\+\", r\" \", ted_talks['transcript'][2451])\n",
    "text = \" \".join(text.split())\n",
    "summarize(text,word_count = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization with Sumy\n",
    "\n",
    "Along with TextRank , there are various other algorithms to summarize text.\n",
    "\n",
    "**sumy** libraray provides you several algorithms to implement Text Summarzation. We are going to implement the below algorithms for summarization using sumy :\n",
    "\n",
    "    LexRank\n",
    "    Luhn\n",
    "    Latent Semantic Analysis, LSA\n",
    "    KL-Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "- Latent Semantic Analysis is a unsupervised learning algorithm that can be used for extractive text summarization.\n",
    "\n",
    "- It extracts semantically significant sentences by applying singular value decomposition(SVD) to the matrix of term-document frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People sometimes ask me, how can you be an astrobiologist if you don't have your own spaceship?Well, what I do is that I study life in those environments on Earth that most closely resemble other interesting places in the universe.\n",
      "These type of findings suggest to me that on Mars, we may find even photosynthetic life inside caves.\n",
      "But even here, well hidden underground, we found a number of different microorganisms, which suggested to me that similarly dry places, like Mars, may be in inhabited.\n"
     ]
    }
   ],
   "source": [
    "import sumy\n",
    "# Import the summarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# Text to summarize\n",
    "original_text = str(text_preprocess(ted_talks['transcript'][2451]))\n",
    "\n",
    "# Parsing the text string using PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "parser=PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "# creating the summarizer\n",
    "lsa_summarizer=LsaSummarizer()\n",
    "lsa_summary= lsa_summarizer(parser.document,3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lsa_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luhn\n",
    "\n",
    "- Luhn Summarization algorithm’s approach is based on TF-IDF (Term Frequency-Inverse Document Frequency). It is useful when very low frequent words as well as highly frequent words(stopwords) are both not significant.\n",
    "\n",
    "- Based on this, sentence scoring is carried out and the high ranking sentences make it to the summary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This one is able to use ocean mist as a source of water, and strikingly lives in the very bottom of a cave, so it has adapted to live with less than 0.1 percent of the amount of light that regular plants need.\n",
      "So I remembered that I usually see fogs in Yungay, so after setting sensors in a number of places, where I remember never seeing fogs or clouds, I reported four other sites much drier than Yungay, with this one, María Elena South, being the truly driest place on Earth, as dry as Mars, and amazingly, just a 15-minute ride from the small mining town where I was born.Now, in this search, we were trying to actually find the dry limit for life on Earth, a place so dry that nothing was able to survive in it.\n"
     ]
    }
   ],
   "source": [
    "# Import the summarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "# text to summarize\n",
    "original_text= str(text_preprocess(ted_talks['transcript'][2451]))\n",
    "\n",
    "# Creating the parser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "parser=PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "#  Creating the summarizer\n",
    "luhn_summarizer=LuhnSummarizer()\n",
    "luhn_summary=luhn_summarizer(parser.document,sentences_count=2)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in luhn_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But since I do not have the 2.5 billion dollars to send my own robot to Mars, I study the most Martian place on Earth, the Atacama Desert.Located in northern Chile, it is the oldest and driest desert on Earth.\n",
      "In the Atacama, there are places with no reported rains in the last 400 years.How do I know this?\n",
      "Because I was born and raised in this desert.\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.kl import KLSummarizer\n",
    "# Our text to perform summarization\n",
    "original_text= str(text_preprocess(ted_talks['transcript'][2451]))\n",
    "# Creating the parser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "parser=PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "# Instantiating the  KLSummarizer\n",
    "kl_summarizer=KLSummarizer()\n",
    "kl_summary=kl_summarizer(parser.document,sentences_count=3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in kl_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexRank\n",
    "\n",
    "- A sentence which is similar to many other sentences of the text has a high probability of being important. The approach of LexRank is that a particular sentence is recommended by other similar sentences and hence is ranked higher.\n",
    "\n",
    "- Higher the rank, higher is the priority of being included in the summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All life on Earth requires water, so in my case I focus on the intimate relationship between water and life in order to understand if we could find life in a planet as dry as Mars.\n",
      "But since I do not have the 2.5 billion dollars to send my own robot to Mars, I study the most Martian place on Earth, the Atacama Desert.Located in northern Chile, it is the oldest and driest desert on Earth.\n",
      "So I remembered that I usually see fogs in Yungay, so after setting sensors in a number of places, where I remember never seeing fogs or clouds, I reported four other sites much drier than Yungay, with this one, María Elena South, being the truly driest place on Earth, as dry as Mars, and amazingly, just a 15-minute ride from the small mining town where I was born.Now, in this search, we were trying to actually find the dry limit for life on Earth, a place so dry that nothing was able to survive in it.\n"
     ]
    }
   ],
   "source": [
    "# Importing the parser and tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "original_text= str(text_preprocess(ted_talks['transcript'][2451]))\n",
    "\n",
    "# Import the LexRank summarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "# Initializing the parser\n",
    "my_parser = PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "# Creating a summary of 3 sentences.\n",
    "lex_rank_summarizer = LexRankSummarizer()\n",
    "lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lexrank_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Among all the above algorithms, Text summarization with spaCy makes sense to me.So I choose the spaCy one for summarization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talks['summary'] = ted_talks['transcript'].apply(text_summary_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>I think you'd have to conclude, if you look at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>And so I'm going to be conducting a course thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>And the truth is, for years I was a little dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>When she came into my life, we were fighting a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>And in the '90s, we have the terrible HIV epid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0      Do schools kill creativity?   \n",
       "1      Averting the climate crisis   \n",
       "2                 Simplicity sells   \n",
       "3              Greening the ghetto   \n",
       "4  The best stats you've ever seen   \n",
       "\n",
       "                                         description  \\\n",
       "0  Sir Ken Robinson makes an entertaining and pro...   \n",
       "1  With the same humor and humanity he exuded in ...   \n",
       "2  New York Times columnist David Pogue takes aim...   \n",
       "3  In an emotionally charged talk, MacArthur-winn...   \n",
       "4  You've never seen data presented like this. Wi...   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "3  If you're here today — and I'm very happy that...   \n",
       "4  About 10 years ago, I took on the task to teac...   \n",
       "\n",
       "                                             summary  \n",
       "0  I think you'd have to conclude, if you look at...  \n",
       "1  And so I'm going to be conducting a course thi...  \n",
       "2  And the truth is, for years I was a little dep...  \n",
       "3  When she came into my life, we were fighting a...  \n",
       "4  And in the '90s, we have the terrible HIV epid...  "
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_talks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All life on Earth requires water, so in my case I focus on the intimate relationship between water and life in order to understand if we could find life in a planet as dry as Mars. So I remembered that I usually see fogs in Yungay, so after setting sensors in a number of places, where I remember never seeing fogs or clouds, I reported four other sites much drier than Yungay, with this one, María Elena South, being the truly driest place on Earth, as dry as Mars, and amazingly, just a 15-minute ride from the small mining town where I was born. Now, in this search, we were trying to actually find the dry limit for life on Earth, a place so dry that nothing was able to survive in it.'"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_talks['summary'][2451]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the model\n",
    "with open(\"/Users/HOME/Desktop/Springboard/TED-Talks/Models/\" + 'ted_summary.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(ted_talks, picklefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Our work started with merging the two datasets ted_main.csv and transcripts.csv and then the data cleaning process, during which we changed the original order of data columns for convenience and date columns from the Unix timestamps into a human readable format timestamp. And then checked for the Null values, duplicates and dropped the duplicated rows. In the Exploratory Data Analysis section, we analyzed the dataset using plots such as bar plots, box plots and histograms. Furthermore, this section has figured out other significant analysis about our dataset, regarding the most viewed talks of all time, the top 10 speakers and speakers occupations. We also made hypotheses to figure out the relation between views and speakers' occupation. From the ANOVA test, we concluded that There is no statistical significant relationship between views and speakers occupation.We also showed interesting statistics about views, comments distribution and proved their relationship using Pearson correlation statistical test. And we also showed TED Talks distribution over years, months and weekdays, and some of them were a bit surprising. During the analysis we figured out the outliers and did not remove them as they are actual data for our exploratory analysis. We figured out the collinear features through a heat map. We also analysed several other pairs for a meaningful correlation but they do not seemed to be strongly correlated. We showed the duration distribution and observed that the short duration TED talks are more famous, it is more likely that people are interested in shorter duration talks because they are able to grasp the talk’s content easily or they don’t have time to watch longer duration talks. We also analyzed the ratings features and visualized the top 10 most funniest, beautiful, inspiring, jaw-dropping and confusing talks of all time. We investigated the TED wordcloud to know about which words are most often used by TED Speakers as well as TED themes and occupations. \n",
    "\n",
    "Next we moved on to the preprocessing step, there we did feature extraction and feature engineering on the dataset. And then we did text preprocessing on the transcript which includes converting all letters to lower or upper case, converting numbers into words or removing numbers, removing punctuations, accent marks and other diacritics, removing white spaces, removing stop words, sparse terms and particular words. We did sentiment analysis on transcript and derived appropriate rating categories for transcripts from rating feature. And then we visualized the ratings categories with respect to sentiment.\n",
    "\n",
    "Next we applied summarization algorithms using spaCy, Gensim and sumy(LexRank,LSA,etc) to extract the summary from the transcript. We found that summarization with spaCy gave good results compared to others for this dataset. \n",
    "\n",
    "**In conclusion, our work led to interesting results, analysis and statistics, but also provided useful tools both for audience and speakers, which allows a better understanding of TED Talks dataset.** \n",
    "\n",
    "This project gave me an opportunity to explore this freely available dataset using NLP and a proper data science pipeline of data wrangling, data analysis, data visualization, prediction, and data storytelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "\n",
    "- Further analysis can be done over the rating column in the dataset to relate the negative comments with topics of TED talk, and find the area of talk which has received more negative feedback. \n",
    "\n",
    "- We can also make some more analysis over topic and area of TED Talk, by combining some other datasets like news article, social media post etc to find for any pattern between how the hot discussed topics over world found from news article and social media post are included in TED talk topics, around the same time frame as of the hot discussion over the world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
